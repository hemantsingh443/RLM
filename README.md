# RLM Engine - Recursive Language Model

An inference engine that enables LLMs to analyze documents of **any size** by treating them as external variables in a sandboxed Python environment. Based on the methodology from [Zhang et al. (MIT CSAIL)](https://arxiv.org/abs/2512.24601).

## How It Works

Instead of stuffing a 10M+ token document into a context window, RLM:

1. **Loads the document** into a persistent Python environment as a `context` variable
2. **The LLM writes code** to explore, search, and extract relevant sections
3. **Recursive sub-agents** can be spawned via `llm_query()` to analyze chunks
4. **Multi-turn iteration** continues until the task is complete

```
┌─────────────────────────────────────────────────────────────┐
│  User Query: "Summarize chapter 3"                          │
└─────────────────┬───────────────────────────────────────────┘
                  ▼
┌─────────────────────────────────────────────────────────────┐
│  Root Agent (LLM)                                           │
│  - Writes: chapters = context.split("Chapter")              │
│  - Writes: summary = llm_query(f"Summarize: {chapters[3]}") │
└─────────────────┬───────────────────────────────────────────┘
                  ▼
┌─────────────────────────────────────────────────────────────┐
│  Docker Sandbox                                             │
│  - Executes code in persistent Python REPL                  │
│  - context variable holds entire document in memory         │
│  - llm_query() spawns sub-agent calls                       │
└─────────────────────────────────────────────────────────────┘
```

## Quick Start

### 1. Setup

```bash
# Clone and enter directory
cd RLM

# Create virtual environment
python -m venv env
source env/bin/activate

# Install dependencies
pip install -r requirements.txt

# Add your OpenRouter API key
echo "OPENROUTER_API_KEY=your-key-here" > .env
```

### 2. Build Docker Image

```bash
docker build -t rlm-sandbox .
```

### 3. Run a Query

```bash
python main.py "What are the main themes?" book.txt
python main.py "Find all function definitions" code.py --type "Python code"
python main.py "Summarize the conclusions" paper.txt --max-turns 20
```

## Features

| Feature | Description |
|---------|-------------|
| **Unlimited Context** | Documents loaded into sandbox memory, not LLM context |
| **Persistent State** | Variables persist across turns |
| **Recursive Agents** | `llm_query()` spawns sub-agents for chunk analysis |
| **Code Execution** | LLM writes Python to search, extract, and process |
| **Safety Limits** | Recursion depth guards, output truncation |

## Project Structure

```
RLM/
├── main.py                 # CLI entry point
├── Dockerfile              # Sandbox container
├── repl_server.py          # Persistent Python REPL
├── rlm/
│   ├── agent.py            # Main orchestration loop
│   ├── prompts.py          # System prompt templates
│   ├── parser.py           # Response parsing
│   └── clients/
│       ├── openrouter.py   # LLM API client
│       └── docker_sandbox.py # Container management
└── test_data/
    └── sample.txt          # Example document
```

## Configuration

| Option | Default | Description |
|--------|---------|-------------|
| `--model` | `xiaomi/mimo-v2-flash:free` | OpenRouter model |
| `--max-turns` | 15 | Maximum conversation turns |
| `--type` | "text document" | Document type hint |
| `--quiet` | false | Suppress verbose output |

## How Recursion Works

The LLM can spawn sub-agents to divide work:

```python
# Example code generated by the root agent:
chunks = [context[i:i+5000] for i in range(0, len(context), 5000)]
summaries = []
for chunk in chunks[:5]:
    # Each llm_query() call spawns a sub-agent
    summary = llm_query(f"Summarize this section:\n{chunk}")
    summaries.append(summary)
final_summary = llm_query(f"Combine these summaries:\n{summaries}")
```

**Safety Limits:**
- Max recursion depth: 3 (configurable via `RLM_MAX_RECURSION_DEPTH`)
- Output truncation: 50,000 chars per execution

## Requirements

- Python 3.11+
- Docker
- OpenRouter API key

## License

MIT
